---
title: "Class 1 - Methods 2"
author: "Pernille Brams feat. Kathrine Schulz"
date: "8/2/2024"
output:
  html_document:
    toc: true
---
HOLOLWOWLOW I made a change here, that I would like to commit to my github repo!
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Setting my root directory to where I have my /data folder etc. (easier for me, but personalise to your own way of working)
knitr::opts_knit$set(root.dir = "/Users/niels/Desktop/University/Second Semester/Methods 2/Github Repo Local/resources/classes/data")

```

### Setup
```{r}
# Make sure this guy is installed/updated (if you've alreadygot rstanarm installed, you just need to load it in using either library() or p_load() as below)
# Load the rest
library(pacman)
pacman::p_load(tidyverse,
               ggpubr,
               ggplot2,
               stringr, 
               rstanarm) # this time I'm just giving you the code
```

### Code from Chapter 1 (not exercises, just in-chapter)
```{r, eval=FALSE}
# Load data
hibbs <- read.table("/Users/niels/Desktop/University/Second Semester/Methods 2/Github Repo Local/resources/classes/data/ElectionsEconomy/data/hibbs.dat", header = TRUE)

# Make scatterplot
plot(hibbs$growth, hibbs$vote, xlab="Average recent growth in personal income",
ylab="Incumbent party's vote share")

# Estimate regression y = a + bx + error
M1 <- stan_glm(vote ~ growth, data=hibbs)

# Add a fitted line to the graph
abline(coef(M1), col="gray") # needs to be run with the plot() code above - running the whole chunk is the easiest way

# Display the fitted model
print(M1)

```

The first column shows estimates: 46.3 and 3.0 are the coefficients in the fitted line, y = 46.3 + 3.0x (see Figure 1.1b). The second column displays uncertainties in the estimates using median absolute deviations (see Section 5.3).

The last line of output shows the estimate and uncertainty of Ïƒ, the scale of the variation in the data unexplained by the regression model (the scatter of the points above and below from the regression line). This is also referred to in the book as residual standard deviation, and it is (like we know 'standard deviation' from Methods 1) a measure of the typical distance that the observed values fall from the regression line.

In Figure 1.1b, the linear model predicts vote share to roughly an accuracy of 3.9 percentage points. This means that on average, the actual values of the dependent variable (vote) deviate from the values predicted by the stan_glm model by about 3.9 percentage points.

### Code for nicer looking plot
```{r}

# Basic plot with ggplot2
ggplot(hibbs, aes(x = growth, y = vote)) +
  geom_point() +  # Add points
  labs(
    x = "Average recent growth in personal income",
    y = "Incumbent party's vote share",
    title = "Relationship between Income Growth and Vote Share",
    subtitle = "Data from Hibbs Dataset"
  ) +
  theme_minimal() +  # Use a minimal theme
  theme(
    plot.title = element_text(hjust = 0.5),  # Center the title
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  ) +
  geom_smooth(method = "lm", se = FALSE, color = "blue")  # Add a linear regression line

```

# Exercises
These first exercises are about learning what simulating data is for.

## Ex. 0.a): In 1-2 sentences (discuss w group or reflect on your own): What does it mean to simulate data?
It means to create your data based on some assumptions of how the data should look. This could be that you have already gotten like 100 observations, but think that this is not enough to have any practical power, thus you based on these observations generate 10.000 observations. 

## Ex. 0.b): Specify some amount of data points (n), some mean and some sd, and use rnorm() to simulate this amount of data points based off of the mean and sd you've set. 
```{r}
set.seed(1998) # setting a seed (in the best year ever??) - this way, even though it's random, you'll get reproducible results next time you run this with this seed

# rnorm() works like: my_simulated_data <- rnorm(n, mean, sd) - now you go!

# your code here
n <- 10000
M <- 34.4
sd <- 10.0
sim_data_age <- rnorm(n, M, sd)
obs_n <- seq(1,n)
sim_df <- tibble(obs_n, sim_data_age)
```

## Ex. 0.c) Talk with your group and note in 1-2 sentences: What do we know of the data drawn from the rnorm()? 
It will follow a normal distribution with our mean at 34.4 and a standard deviation of 10.0. There will also be 10,000 observations.

## Ex. 0.d) Data are worth 10000% more if visualised, so let's visualise. Using ggplot, make a histogram visualising the data you simulated.
```{r}
ggplot(data = sim_df) + 
  geom_histogram(aes(x = sim_data_age), binwidth = 0.5, stat = "bin", fill = "skyblue")
```

## Ex. 0.e) Calculate the empirical mean and empirical sd (just fancy words for "calculate the mean and sd of the simulated data") - are they different from the ones you've set? and why might that be?
```{r - Mean & SD}
calc_mean_sim <- mean(sim_data_age)
calc_sd_sim <- sd(sim_data_age)

```

## Exercises from ROS
### Ex. 1.2
Sketching a regression model and data: Figure 1.1b shows data corresponding to the fitted line y = 46.3 + 3.0x with residual standard deviation 3.9, and values of x ranging roughly from 0 to 4%.
("Sketch" or "graph"= means simulate i.e. skething data means make some data yourself :) )

#### Ex. 1.2.a) Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 3.9.
#### Ex. 1.2.b) Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 10.
* If you're not sure how to approach this exercise, try and go by the piecemeal hint-instruction in the bottom of this doc :)
```{r - ex.1.2a}
# generate perfect x and y data
int_1.2 <- 30
slope_1.2 <- 10
sd_1.2 <- 3.9
ex1.2_x_range <- seq(0, 4, length.out = 100)
ex1.2_y_range <- int_1.2+slope_1.2*ex1.2_x_range
ex_1.2_perf_df <- tibble(ex1.2_x_range, ex1.2_y_range)

# plot it and see that it is a perfect line
ggplot(ex_1.2_perf_df) +
  geom_point(aes(ex1.2_x_range, ex1.2_y_range))

# add random error
ex1.2_y_random_error <- ex1.2_y_range + rnorm(length(ex1.2_x_range), mean = 0, sd = 3.9)

# make it a tibble
ex_1.2_imp_df <- tibble(ex1.2_x_range, ex1.2_y_random_error)

# plot it to see that we are now not on a perfect line - as normal data would look like 
ggplot(ex_1.2_imp_df) +
  geom_point(aes(ex1.2_x_range, ex1.2_y_random_error))

# additionally fitting a linear regression
lm(ex1.2_y_random_error~ex1.2_x_range, data = ex_1.2_imp_df)

# Ex 1.2b
ex1.2b_y_random_error <- ex1.2_y_range + rnorm(length(ex1.2_x_range), mean = 0, sd = 10)
ex_1.2b_imp_df <- tibble(ex1.2_x_range, ex1.2b_y_random_error)
ggplot(ex_1.2b_imp_df) +
  geom_point(aes(ex1.2_x_range, ex1.2b_y_random_error))

# additionally fitting a linear regression
lm(ex1.2b_y_random_error~ex1.2_x_range, data = ex_1.2b_imp_df)
```


### Ex. 1.5 Goals of regression: Give examples of applied statistics problems of interest to you in which the goals are:
#### (a) Forecasting/classification
Classifying metaphors in reddit comments is an interesting NLP problem, which in someway requires classification right?

#### (b) Exploring associations
For applied cogsci we're looking at developing a cool weekly planner, and it would be interesting to see if having 3D-blocks is associated with a better retention of your weekly plans.

#### (c) Extrapolation
If a students grades continue from primary school to high school to uni etc.

#### (d) Causal inference
Are teachers slide quality directly influencing students grades?

### Ex. 2.3 Data processing: Go to the data folder Names, find the file allnames_clean.csv and use this to make a graph similar to Figure 2.8, but for girls. 

```{r}
all_names_df <- read.csv("/Users/niels/Desktop/University/Second Semester/Methods 2/Github Repo Local/resources/classes/data/Names/data/allnames_clean.csv")
girl_names <- all_names_df %>%
  filter(sex == "F")

# making the all years into only 1 column with the rows indicating which year for which name.
long_girls_data <- girl_names %>%
  pivot_longer(
    cols = starts_with("X"), # This selects all columns that start with 'X'
    names_to = "year",
    values_to = "count",
    names_prefix = "X", # This removes the 'X' prefix from your year columns
    names_transform = list(year = as.integer) # Convert the year from character to integer
  )

# finding last letter in name and adding it as a column
long_girls_data <- long_girls_data %>% mutate(last_letter = str_sub(long_girls_data$name, -1)) 

# removing NA
long_girls_data <- long_girls_data %>% drop_na()

unique(long_girls_data$last_letter)

# Calculating yearly total by grouping by last letter
letter_counts <- long_girls_data %>% 
  group_by(year, last_letter) %>% 
  summarize(count = sum(count), .groups = 'drop')

# Testing for duplicates (found none)
dup_letters <- letter_counts %>%
  group_by(year, last_letter) %>% 
  filter(n()>1) %>% 
  ungroup
#print(dup_letters) - found no duplicates

# Calculating total letters per year
year_letters <- long_girls_data %>%
  group_by(year) %>% 
  summarize(year_total_letters = sum(count), .groups = 'drop')

# Join the data frames correctly to calculate the percentages
letter_counts <- letter_counts %>%
  left_join(year_letters, by = "year") %>%
  mutate(year_percentage = (count / year_total_letters) * 100) %>%
  ungroup()

# Plotting it!
ggplot(letter_counts, aes(x = year, y = year_percentage, group = last_letter, color = last_letter)) +
  geom_line() +
  theme_minimal() +
  labs(
    title = "Last Letters of Girls Names",
    x = "Year",
    y = "Percentage of All Girls Names Of Year"
  ) +
  theme(legend.position = "none") +
  geom_text(data = )
```


### Ex. 2.7 (b) Reliability and validity: Give an example of a scenario of measurements that have reliability but not validity.

# Pernille's additional notes to Chapter 1 in ROS (not mandatory to read at all)
I probably wont keep making these notes this formatted but hey, here's to get you started on methods 2 transitioning from methods 1 :) They are just notes I took when skimming through the chapter.

Some of the text below mentions a lot of new words. But e.g. learning what bayesian approaches are in detail are NOT the point of methods 2. So don't worry if it doesn't make sense yet. It's not supposed to, the focus is on Bayesian in Methods 3 and 4 - and see the TL;DR on the bottom of this doc.

## The chapter
The chapter basically explains why regression is cool: it's a method used to explore a relationship between a dependent variable and one or more independent ones. The basic idea is to find the best straight line that fits some data points. Once we have the line, and if it fits well (but not too well) we can use it to make predictions. For example, if you know someone's height (X), you can plug it into the equation Y = a+bX+e to predict their weight (Y).
- Figure 1.1 in the book shows a) incumbent party's (i.e. the political party in power at time of election) vote share in some US elections, and b) a linear regression fit to these data.

## Explanations to couple the chapter with Methods 1 and the cogsci bsc in general
Note that the book uses the 'stan_glm' function to fit models. In Methods 1 we only used lm() and lmer(), and a few of you may have used glm() in your projects for e.g. logistic regressions or multinomial. 'stan_glm' is a function from the rstanarm package in R, which is used for *Bayesian generalized linear modeling.* The package allows for fitting of what we call Bayesian models using Stan, which is a programming language designed specifically for Bayesian modelling. It uses technique like Markov Chain Monte Carlo (MCMC) to sample from probability distributions. Don't think too hard about it- you'll learn more about the details of it. For now, we just use stan_glm() to fit lines to stuff.

*1. So many new terms already!! What is the difference between "Frequentist" and "Bayesian" approaches?*
Let's take lm() and stan_glm() as our two examples to explain this. The former is frequentist and proud, the latter Bayesian and proud.

**Frequentist:**
We can use lm() for linear regression. The lm() function estimates coefficients based on the data, aiming to minimise the error between the predicted and observed values (Methods 1 stuff). We express uncertainty in the lm() through confidence intervals and p-values, which are based on the concept of repeating an experiment indefinitely. The lm() cannot incorporate any prior knowledge, and as such, the analysis we can do with this one is based solely on the current data.

**Bayesian (if you're curious to know. More will come in methods 4):**
In the Bayesian framework, we can combine prior beliefs with the data we have at hand to update our knowledge about the estimates (in lm() we can only build a model on the data we have, so if the data are flawed or just straight up wrong, then we're doomed to build a really bad model).

The function stan_glm() is the lm() counterpart, just in a Bayesian framework instead of frequentist. stan_glm() incorporates so-called "prior distributions" for the estimates and uses data to update these "beliefs", resulting in so-called "posterior distributions" (which is essentially our results; it is what the model returns to us after calculating a bunch of smart stuff for us using MCMC). In stan_glm, uncertainty is NOT expressed through p-values, but through "credible intervals" (confidence intervals better cousin) derived from the posterior distributions, providing a direct probabilistic interpretation.
Prior Information: Integrates prior knowledge or beliefs through the use of prior distributions, which are then updated with the data.

**1 What is the difference between confidence intervals and credible intervals?**
- As we have seen in Methods 1, statisticians love giving names to things :)
- One could explain the difference between the two in a very long monologue (cause they *are* fundamentally statistically different), but the key point is that a) the two terms come from two different frameworks of stats, but b) have the same overall purpose: Wider intervals (be it confidence or credible) generally indicate more uncertainty about a value. Narrow means we can be more certain about a value.

*2. Why do we all of a sudden use 'stan_glm()' and not just good ol' lm()/lmer() from Methods 1?*
The function 'stan_glm' and Bayesian models in general can be more flexible in handling complex models, such as those with non-normal error distributions or hierarchical structures, that go beyond the scope of lm() and lmer(). Furthermore, in cases with small sample sizes, rare events, or multicollinearity, Bayesian methods can give us more robust and reliable estimates. You'll use stan and bayesian in general a lot more on Methods 3 and 4, so look at the use of it here as a fancy but basically the same as lm()-work, letting you get a taste of the functions. When you run stan_glm(), you will see that it samples via MCMC, which is algorithms that sample from posterior distributions of the parameters. It offers full probabilistic inference for the model parameters, including credible intervals and posterior predictive checks, which can be more informative than point estimates and confidence intervals in traditional GLMs. If all this are new words to you, good - because we haven't taught you what all that means yet fully.

*3. What is a generalised linear model (GLM) and how is it different from the lm() / lmer()'s we're used to from Methods 1?*
A Generalised Linear Model (GLM) is "just" an extension of the traditional linear model (lm() in R for example) to accommodate response variables that are not normally distributed.

You might remember that in lm(), the response is assumed to be a linear function of the predictors with normally distributed errors (remember the lovely assumption checks?). GLMs relax these constraints in two key ways:
- Different distributions: GLMs allow for response variables to follow different distributions (like binomial for binary data (data with an outcome like TRUE/FALSE, SUCCESS/FAIL, etc.), the so-called Poisson distribution for count data (data with e.g. count of friends, count of seedlings from a field of plants, etc.).
- Link function: GLMs use a so-called link function to model the relationship between the predictors and the mean of the response distribution. You'll learn more about what this is on later semesters, but the link function basically just converts (transforms) the expected value of some response variable so that it can be modeled as a linear combination of predictors, regardless of whatever scale / constraints it came from. In linear regression (lm()), we predict the response variable directly from a linear combination of predictors. However, for many types of data (like counts, binary outcomes), this direct prediction is not appropriate because the response variable has constraints (e.g., it must be positive, or between 0 and 1). So this is what we use link functions for - and this link function can be linear (like in lm()), logistic (for binary outcomes), log (for count data), etc.

Now, the lmer() model as we know from Methods 1 extends lm() by allowing for both fixed and random effects. This is useful for hierarchical or grouped data as we've seen before (see slides from Methods 1 if you need a refresher on linear mixed effects models (lmers). Thing is that GLMs can ALSO be extended to include random effects (GLMMs), so doing GLMs is basically just a more flexible way of doing the stuff we did on Methods 1. It is also cooler. And more buzzwordy. 

TL;DR: GLM/GLMMs can do everything lm() and lmer() can do, just better and you'll iteratively learn why it's better across Methods 2-3-4, trust me :) 

## Piecemeal hints
### Ex. 1.2a) stepwise-instruction
The question says: Sketch hypothetical data with the same range of x but corresponding to the line y = 30 + 10x with residual standard deviation 3.9. We're given a linear regression equation that specifies what y should be, and we're told what the x should be from and to in the question and the info from 1.2). Here's 8 smaller steps on how to solve it, step by step: 

#### 1) First, define the regression model parameters (intercept, slope, error) (hint: just make variables called 'intercept' and so on and define the number)

#### 3) Generate X values in the specified range using seq()

#### 4) Calculate y values based on the given regression and your newly created x-values without the error term, which gives you the y get the perfect fit line

#### 5) Add random error to the line we fit above: we can introduce variability by adding some normally distributed random errors to the y values. This can be done using rnorm() as in the 0-exercises, with mean 0 and standard deviation equal to the residual standard deviation (3.9) for instance.

#### 6) Prepare data for plotting: get the x-values and y-values in a dataframe to prepare for plotting w. ggplot

#### 7) Now, plot to check it out

#### 8) Optional: Fit and review a regression model: As an additional step, you can fit a linear regression model to the generated data using stan_glm() or any other fitting function to see how closely the estimated parameters match the ones used to generate the data. 
